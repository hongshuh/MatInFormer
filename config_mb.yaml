fold_num: 0
dataset_name: 'matbench_mp_e_form'

# train_data: '/home/hongshuh/space_group_transformer/data/mp.csv'
# test_data: '/home/hongshuh/space_group_transformer/data/wbm.csv'

# pretrain_model: null #'/home/hongshuh/space_group_transformer/runs_contrast/May22_19-50-45/best.pth'
# pretrain_model: '/home/hongshuh/space_group_transformer/runs_contrast/May29_10-18-35/best.pth' # Predict lattice
# pretrain_model: '/home/hongshuh/space_group_transformer/runs_contrast/Jun19_19-07-21/best.pth' # Predict lattice
# pretrain_model: '/home/hongshuh/space_group_transformer/runs_contrast/Jun15_13-26-01/checkpoint.pth' # open catalyst
pretrain_model: null
vocab_path: '/home/hongshuh/space_group_transformer/vocab.json'
device: 'cuda:1'
epochs: 200
batch_size: 64
lr: 0.000005
weight_decay: 0.0001
warmup_ratio: 0.05
robust: False
task: 'regression'
max_element: 20
hidden_size: 4096 #1536,768
max_position_embeddings: 40            # max position embeddings of Transformer
blocksize: 34                          # max length of sequences after tokenization
num_attention_heads: 16                 # number of attention heads in each hidden layer
num_hidden_layers: 16                   # number of hidden layers
hidden_dropout_prob: 0.1                # hidden layer dropout
attention_probs_dropout_prob: 0.1       # attention dropout
mlm_probability: 0.15                   # masked probability in mlm